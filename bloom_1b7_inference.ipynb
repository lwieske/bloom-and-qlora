{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lwieske/bloom-quantization-and-qlora-finetuning/blob/main/bloom_1b7_inference.ipynb)\n",
        "[![Open in SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/lwieske/bloom-quantization-and-qlora-finetuning/blob/main/bloom_1b7_inference.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BLOOM 1B7 Inference with Nvidia T4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-03T16:33:09.226833Z",
          "iopub.status.busy": "2023-12-03T16:33:09.225995Z",
          "iopub.status.idle": "2023-12-03T16:33:26.708129Z",
          "shell.execute_reply": "2023-12-03T16:33:26.706945Z",
          "shell.execute_reply.started": "2023-12-03T16:33:09.226799Z"
        },
        "id": "_UyyuMGnCPjA",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip -q install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-03T16:33:26.710511Z",
          "iopub.status.busy": "2023-12-03T16:33:26.710215Z",
          "iopub.status.idle": "2023-12-03T16:33:43.177908Z",
          "shell.execute_reply": "2023-12-03T16:33:43.176975Z",
          "shell.execute_reply.started": "2023-12-03T16:33:26.710484Z"
        },
        "id": "pBOE-h8sbrNK",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-03T16:33:43.179663Z",
          "iopub.status.busy": "2023-12-03T16:33:43.179300Z",
          "iopub.status.idle": "2023-12-03T16:33:43.185046Z",
          "shell.execute_reply": "2023-12-03T16:33:43.184221Z",
          "shell.execute_reply.started": "2023-12-03T16:33:43.179628Z"
        },
        "id": "vziwd2UuCYGl",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#MODEL      = \"bigscience/bloom-560m\"\n",
        "#MODEL      = \"bigscience/bloom-1b1\"\n",
        "MODEL      = \"bigscience/bloom-1b7\"\n",
        "#MODEL      = \"bigscience/bloom-3b\"\n",
        "#MODEL      = \"bigscience/bloom-7b1\"\n",
        "\n",
        "TOKENS     = 20\n",
        "\n",
        "DEVICE     = \"cuda:0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-03T16:33:43.202355Z",
          "iopub.status.busy": "2023-12-03T16:33:43.202018Z",
          "iopub.status.idle": "2023-12-03T16:35:43.973028Z",
          "shell.execute_reply": "2023-12-03T16:35:43.972073Z",
          "shell.execute_reply.started": "2023-12-03T16:33:43.202322Z"
        },
        "id": "W2EZhNQ66EAd",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "pretrained_model = AutoModelForCausalLM.from_pretrained(MODEL)\n",
        "\n",
        "pretrained_model = pretrained_model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-12-03T16:35:43.974501Z",
          "iopub.status.busy": "2023-12-03T16:35:43.974215Z",
          "iopub.status.idle": "2023-12-03T16:35:45.073154Z",
          "shell.execute_reply": "2023-12-03T16:35:45.072163Z",
          "shell.execute_reply.started": "2023-12-03T16:35:43.974478Z"
        },
        "id": "WWgahTxOcXEM",
        "outputId": "572b89cc-b3c6-42fa-9657-35a54476540b",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Dec  4 10:03:22 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P0    27W /  70W |   6675MiB / 15360MiB |     33%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-03T16:35:45.075209Z",
          "iopub.status.busy": "2023-12-03T16:35:45.074797Z",
          "iopub.status.idle": "2023-12-03T16:35:46.443819Z",
          "shell.execute_reply": "2023-12-03T16:35:46.442667Z",
          "shell.execute_reply.started": "2023-12-03T16:35:45.075174Z"
        },
        "id": "aU0awofs84q7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "tokenizer           = AutoTokenizer.from_pretrained(MODEL)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_fR4WttssU9",
        "outputId": "bf73f7f6-139d-41f7-e323-083a1e230a0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Dec  4 10:03:23 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P0    26W /  70W |   6675MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qtc1gbK39Hp7"
      },
      "source": [
        "## Inference with Pretrained BLOOM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-03T16:35:46.445908Z",
          "iopub.status.busy": "2023-12-03T16:35:46.445465Z",
          "iopub.status.idle": "2023-12-03T16:35:46.453059Z",
          "shell.execute_reply": "2023-12-03T16:35:46.451983Z",
          "shell.execute_reply.started": "2023-12-03T16:35:46.445862Z"
        },
        "id": "jak6FzpvFTHk",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def generate_output_tokens(model, input_tokens):\n",
        "    outputs = model.generate(\n",
        "        input_ids=input_tokens[\"input_ids\"],\n",
        "        attention_mask=input_tokens[\"attention_mask\"],\n",
        "        max_new_tokens=TOKENS,\n",
        "        repetition_penalty=1.5,\n",
        "        early_stopping=False,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-12-03T16:35:46.454742Z",
          "iopub.status.busy": "2023-12-03T16:35:46.454391Z",
          "iopub.status.idle": "2023-12-03T16:35:56.957515Z",
          "shell.execute_reply": "2023-12-03T16:35:56.956523Z",
          "shell.execute_reply.started": "2023-12-03T16:35:46.454706Z"
        },
        "id": "3BAYg7czFYeK",
        "outputId": "d0d18c80-4473-4d84-d89d-19e2021a5756",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['What is the meaning of life? What does it mean to be human, and what do we need in order for us not only survive']\n"
          ]
        }
      ],
      "source": [
        "input_words = \"What is the meaning of life?\"\n",
        "\n",
        "input_tokens = tokenizer(input_words, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "pretrained_output_tokens = generate_output_tokens(pretrained_model, input_tokens)\n",
        "\n",
        "pretrained_output_words = tokenizer.batch_decode(pretrained_output_tokens, skip_special_tokens=True)\n",
        "\n",
        "print(pretrained_output_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttIVW6j7ssU_",
        "outputId": "0aa7caf5-7913-4d79-fe44-832e332f5a60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Dec  4 10:03:24 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P0    71W /  70W |   6743MiB / 15360MiB |     94%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30588,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
